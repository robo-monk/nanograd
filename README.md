# nanograd

code & notes from Andrej Karpathy's [lecture on backpropagation](https://www.youtube.com/watch?v=VMj-3S1tku0)

# Backpropagation

Efficiently evaluate the gradient of a function with respect of the weights.

In a neural network context, that function is usually the loss function. That allows us to tune the weights of the loss function to minimize the function & improve the accuracy of the nn.
